{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))\n",
    "\n",
    "# only for .ipynb because relative imports don't work\n",
    "root_path = (DATA_PATH.parent) \n",
    "os.chdir(str(root_path))\n",
    "\n",
    "import src.training.postprocessing as pp\n",
    "import src.training.pre_training as t\n",
    "import src.training.plotting as p\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, plot_confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn ensemble\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "\n",
    "# import models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and feature assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = t.get_complete_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_complete.drop(['id', 'name', 'artist_id', 'generation', 'song_id', 'primary_artist_id', 'lyrics_skipped', 'lyrics_stored'], axis=1)\n",
    "print(list(df))\n",
    "\n",
    "# Song popularity\n",
    "df.iloc[:, 0] = df.iloc[:, 0].apply(t.binary_popularity)\n",
    "\n",
    "# Artist popularity\n",
    "df.iloc[:, -2] = df.iloc[:, -2].apply(t.binary_popularity)\n",
    "\n",
    "# Artist genres\n",
    "df.iloc[:, -1] = df.iloc[:, -1].apply(t.encode_genre)\n",
    "\n",
    "max_followers = df[\"followers\"].max()\n",
    "df[\"followers\"] = df[\"followers\"].apply(lambda x: x / (max_followers / 100))\n",
    "\n",
    "X_all, y = RandomUnderSampler(random_state=42).fit_resample(df.iloc[:, 1:], df.iloc[:, 0])\n",
    "print(pp.count_distribution(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pear_corr = df.corr(method='pearson')\n",
    "# plt.imshow(pear_corr, cmap='PuBuGn', extent=[\"test\", \"r\", \"t\", \"s\"])\n",
    "# plt.show()\n",
    "# # print(pear_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(\n",
    "    X_all, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_test_dict = dict()\n",
    "X_train_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataframe into model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music features\n",
    "X_train_dict['music'] = X_train_all.iloc[:, 1:15]\n",
    "X_test_dict['music'] = X_test_all.iloc[:, 1:15]\n",
    "\n",
    "# lyrics features\n",
    "X_train_dict['lyrics'] = X_train_all.iloc[:, 15:19]\n",
    "X_test_dict['lyrics'] = X_test_all.iloc[:, 15:19]\n",
    "\n",
    "# artist features\n",
    "X_train_dict['artist'] = X_train_all.iloc[:, -3:]\n",
    "X_test_dict['artist'] = X_test_all.iloc[:, -3:]\n",
    "\n",
    "# all features\n",
    "X_train_dict['all'] = X_train_all\n",
    "X_test_dict['all'] = X_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "pear_corr = df.corr(method='pearson')\n",
    "\n",
    "plt.imshow(pear_corr, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for classifiers\n",
    "clf_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=250)\n",
    "rf.fit(X_train_dict['music'], y_train)\n",
    "\n",
    "clf_list.append((rf, 'music'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_clf = GaussianNB()\n",
    "gaussian_clf.fit(X_train_dict['lyrics'], y_train)\n",
    "\n",
    "clf_list.append((gaussian_clf, 'lyrics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_dict['artist'], y_train)\n",
    "clf_list.append((knn_clf, 'artist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_clf = nn_clf = RandomForestClassifier(n_estimators=100)\n",
    "complete_clf.fit(X_train_dict['all'], y_train)\n",
    "\n",
    "clf_list = [(complete_clf, 'all')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_list)\n",
    "\n",
    "predictions = np.asarray([clf.predict(X_test_dict[X_type]) for clf, X_type in clf_list])\n",
    "\n",
    "weights = None\n",
    "pred_avg = np.average(predictions, axis=0, weights=weights).round()\n",
    "\n",
    "print(\"Weighted accuracy: \" + str(round(accuracy_score(y_test, pred_avg), 4)))\n",
    "print(\"Weighted precision: \" + str(round(precision_score(y_test, pred_avg, average=\"weighted\"), 4)))\n",
    "print(\"Weighted recall: \" + str(round(recall_score(y_test, pred_avg, average=\"weighted\"), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled conf matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import itertools\n",
    "\n",
    "# y_pred = rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, pred_avg, normalize='true')\n",
    "\n",
    "classes = [\"0\", \"1\"]\n",
    "# classes = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "im = ax.imshow(cm, vmin=0, vmax=1, cmap=\"Blues\")\n",
    "ax.set_title(\"Random forest on complete dataset\")\n",
    "tick_marks = np.arange(len(classes))\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_yticklabels(classes)\n",
    "\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    text_color = \"white\" if cm[i, j] > 0.8 else \"black\"\n",
    "    ax.text(j, i, format(cm[i, j], '.2f'),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=text_color)\n",
    "\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=\"Blues\", norm=plt.Normalize(vmin=0, vmax=1))\n",
    "plt.colorbar(sm)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = complete_clf.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in complete_clf.estimators_], axis=0)\n",
    "\n",
    "feature_list = list(map(lambda feat: \"loudness\" if feat == \"loadness\" else feat, list(X_all)))\n",
    "feature_list = list(map(lambda feat: \"artist_popularity\" if feat == \"popularity\" else feat, feature_list))\n",
    "\n",
    "forest_importances = pd.Series(importances, index=feature_list).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI on V2\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    complete_clf, X_test_dict['all'], y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "\n",
    "\n",
    "feature_list = list(map(lambda feat: \"loudness\" if feat == \"loadness\" else feat, list(X_all)))\n",
    "feature_list = list(map(lambda feat: \"artist_popularity\" if feat == \"popularity\" else feat, feature_list))\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_list).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDA on V2 binary\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distr\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Distribution of predicted popularity\")\n",
    "ax.set_xlabel(\"popularity\")\n",
    "ax.set_ylabel(\"count\")\n",
    "\n",
    "plt.bar(list(set(pred_avg)), pp.count_distribution(pred_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, cax = plt.subplots(figsize=(8, 8)) # subplot for larger size\n",
    "IC = type('IdentityClassifier', (), {\"predict\": lambda i : i, \"_estimator_type\": \"classifier\"})\n",
    "cax.set_title(\"Ensemble Classifier\", fontsize=15)\n",
    "plot_confusion_matrix(IC, y_test, pred_avg, cmap=plt.cm.Blues,normalize=\"true\",values_format=\".2f\",ax=cax)\n",
    "plt.show()\n",
    "\n",
    "fig, cax = plt.subplots(figsize=(8, 8)) # subplot for larger size\n",
    "IC = type('IdentityClassifier', (), {\"predict\": lambda i : i, \"_estimator_type\": \"classifier\"})\n",
    "cax.set_title(\"Ensemble Classifier\", fontsize=15)\n",
    "plot_confusion_matrix(IC, y_test, pred_avg, cmap=plt.cm.Blues,normalize=None,values_format=\".2f\",ax=cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of artist popularity on song popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take popular songs from X_test and set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Songs with from popular artist\n",
    "X_test_popular = X_test_all[X_test_all['popularity'] > 0]\n",
    "\n",
    "# set popularity to 0\n",
    "X_test_popular.loc[:, 'popularity'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite test dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dict['music'] = X_test_popular.iloc[:, 1:15]\n",
    "\n",
    "X_test_dict['lyrics'] = X_test_popular.iloc[:, 15:19]\n",
    "\n",
    "X_test_dict['artist'] = X_test_popular.iloc[:, -3:]\n",
    "\n",
    "X_test_dict['all'] = X_test_popular\n",
    "print(len(list(X_test_popular)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict popularity again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new = np.asarray([clf.predict(X_test_dict[X_type]) for clf, X_type in clf_list])\n",
    "\n",
    "weights = None\n",
    "pred_avg_new = np.average(predictions_new, axis=0, weights=weights).round()\n",
    "print(list(pred_avg)[:10])\n",
    "print(pred_avg_new[:10])\n",
    "\n",
    "print(\"Weighted accuracy: \" + str(round(accuracy_score(y_test, pred_avg), 4)))\n",
    "print(\"Weighted precision: \" + str(round(precision_score(y_test, pred_avg, average=\"weighted\"), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for differences in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = list(zip(filter(lambda x: x == 1, y_test), pred_avg_new))\n",
    "\n",
    "# list of popular songs that are predicted unpopular\n",
    "l = [x for x, y in zipped if x == 1 and y == 0]\n",
    "\n",
    "print(\"Popular predictions\", len(l), \"/\", len(zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distr\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Distribution of predicted popularity\")\n",
    "ax.set_xlabel(\"popularity\")\n",
    "ax.set_ylabel(\"count\")\n",
    "\n",
    "plt.bar(list(set(pred_avg_new)), pp.count_distribution(pred_avg_new))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79c01d956d8d580ff50d5e99a3a6d3d709ffed313f15b1c7fdf81c9e53683cc0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
